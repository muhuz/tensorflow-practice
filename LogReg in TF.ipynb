{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_moons\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "moons = make_moons()\n",
    "\n",
    "train, labels = moons[0], moons[1].reshape((-1, 1))\n",
    "m, n = train.shape\n",
    "train_std = (train - np.mean(train, axis=0))/np.std(train, axis=0)\n",
    "norm_bias = np.c_[np.ones((m, 1)), train_std]\n",
    "\n",
    "x_train = norm_bias[:50]\n",
    "y_train = labels[:50]\n",
    "\n",
    "x_test = norm_bias[50:]\n",
    "y_test = labels[50:]\n",
    "\n",
    "n_epoch = 1000\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = tf.constant(norm_bias, dtype=tf.float32, name=\"X\")\n",
    "# y = tf.constant(labels.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "# theta = tf.Variable(tf.random_uniform([n+1, 1], -1.0, 1.0), name='theta')\n",
    "# prob = tf.sigmoid(-tf.matmul(X, theta))\n",
    "# pred = tf.round(prob)\n",
    "\n",
    "# logloss = -tf.reduce_mean(pred * tf.log(prob) + (1-pred) * tf.log(1-prob))\n",
    "# gradient = 1/m * tf.matmul(tf.transpose(X), prob - pred)\n",
    "# training_op = tf.assign(theta, theta-learning_rate * gradient)\n",
    "\n",
    "# init = tf.global_variables_initializer()\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(init)\n",
    "    \n",
    "#     for epoch in range(n_epoch):\n",
    "#         if epoch % 100 == 0:\n",
    "#             print(\"Epoch:\", epoch, \"LogLoss = \", logloss.eval())\n",
    "#         sess.run(training_op)\n",
    "        \n",
    "#         final_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 LogLoss =  0.5354365\n",
      "Epoch: 100 LogLoss =  0.39628494\n",
      "Epoch: 200 LogLoss =  0.10696957\n",
      "Epoch: 300 LogLoss =  0.3750423\n",
      "Epoch: 400 LogLoss =  0.09366602\n",
      "Epoch: 500 LogLoss =  0.3283423\n",
      "Epoch: 600 LogLoss =  0.11879829\n",
      "Epoch: 700 LogLoss =  0.11819396\n",
      "Epoch: 800 LogLoss =  0.105071306\n",
      "Epoch: 900 LogLoss =  0.14723867\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32, [None, n+1])\n",
    "y = tf.placeholder(tf.float32, [None, 1])\n",
    "theta = tf.Variable(tf.random_uniform([n+1, 1], -1.0, 1.0), name='theta')\n",
    "prob = tf.sigmoid(-tf.matmul(X, theta))\n",
    "pred = tf.round(prob)\n",
    "\n",
    "logloss = -tf.reduce_mean(pred * tf.log(prob) + (1-pred) * tf.log(1-prob))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(logloss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        rand_index = np.random.choice(100)\n",
    "        rand_x = [norm_bias[rand_index]]\n",
    "        rand_y = [labels[rand_index]]\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch:\", epoch, \"LogLoss = \", sess.run(logloss, feed_dict={X: rand_x, y: rand_y}\n",
    "                                                         ))\n",
    "        sess.run(training_op, feed_dict={X: rand_x, y: rand_y})\n",
    "        \n",
    "        final_theta = theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.8430817 ],\n",
       "       [-1.1393532 ],\n",
       "       [-0.26066798]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = sigmoid(np.matmul(norm_bias, final_theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(probs, cutoff):\n",
    "    return np.array([1 if i > cutoff else 0 for i in probs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = predict_class(probs, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.56"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum([1 for i in preds.reshape((-1, 1)) - labels if i != 0])/preds.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression().fit(norm_bias[:50, :], labels[:50].reshape(-1, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_preds = clf.predict(norm_bias[50:, :]).reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(sk_preds - labels[50:])/sk_preds.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
